---
layout: post
title: "ABTest的流程"
date: 2021-04-28
description: "ABTest的流程"
tag: 数据分析
mermaid: true
katex: true
---

AB Test的流程如图所示：
![](/assets/2021-04-28-abtest-process1.png)

## 明确AB Test的目标和假设

把产品 / 业务的变化作为原因，把业务目标变成结果，我们就把业务问题转换成了因果推断。

>以一款按月付费的音乐 App 要提高营收为例，该如何确定目标和假设。
>
>**首先，分析问题，确定想要达到的结果。想要提高营收，我们首先得清楚问题出在哪里。**
>
>这个时候，我们可以进行数据分析。比如，和竞品进行对比分析后发现，我们 App 的用户留存率低于行业平均水平。因此，用户留存率就是我们这款 App 目前存在的问题。
>
>**其次，提出解决业务问题的大致方案。**
>
>影响用户留存的原因有很多种。比如，内容是否足够丰富，能满足不同用户的音乐需求？产品是否有足够多的便利功能，可以给用户更好的使用体验？App 的开启和运行速度是否足够流畅？通过进一步的分析发现，我们的产品在歌曲库的内容和丰富程度上，都在行业平均水平之上，而且 App 的运行也十分流畅，但是缺少一些便利的产品功能。所以，我们提出的大致解决方案就是，要通过增加产品功能来提升用户留存。
>
>**最后，从大致的解决方案中提取出具体的假设。**
>
>那针对这款音乐 App，可以增加什么具体的产品功能呢？你可能会想到，在每个专辑 / 歌单播放完成后增加“自动播放下一个专辑 / 歌单”的功能，以此来提升用户留存。这样一来，我们就通过三个步骤基本确定了目标和假设。为什么说是“基本确定”了呢？因为确定目标和假设到这里还没有完全完成。要注意了，我们在上面确定目标和假设的时候其实还忽略了一个隐形的坑：这个假设中的“提升用户留存”还不能算是一个好的目标。因为这个假设还不够具体，目标没有被量化，而没有量化就没有办法提升。所以在这里，我们还需要做的就是量化“用户留存率”这个概念。
>
>在按月付费的音乐 App 这个案例中，用户只要每个月按时付费续订，就是留存。所以，我们可以把用户留存定义为下个月的续订率，这样我们就把假设变得更加具体，并且目标可被量化。那我们优化后，这个 A/B 测试的假设就变成了：在每个专辑 / 歌单播放完成后增加“自动播放下一个专辑 / 歌单”的功能，可以提升用户下个月的续订率。
>
>根据大佬的经验，什么是好的假设有一个标准：
>
>![](/assets/2021-04-28-abtest-process2.png)

根据上述的过程，我们要确定好AB Test的目标和假设。

## 选取指标

我们需要重视指标的选取，因为好的指标可以帮助我们衡量自身的进步与实行问责制。通常，指标是分层级的，用OKR系统来举例，我们可以把指标分为：

1. **目标指标**：又叫做核心指标/ 北极星指标：这个指标是一个长期性质的，体现了我们终极关注的是什么。
2. **驱动指标**：又叫观测/评价指标：他们是短期的，可观察，易测量的，体现的是如何做可以让我们的试验成功。观测指标又分为两种：绝对值类指标和比率类指标：
   - **绝对值类指标。**我们平常直接计算就能得到的单个指标，不需要多个指标计算得到。一般都是统计该指标在一段时间内的均值或者汇总值，比如DAU，平均停留时长等。这类指标一般较少作为AB测试的观测指标。
   - **比率类指标。** 与绝对值类指标相对应，我们不能直接计算得到，而是通过多个指标计算得到。比如某页面的点击率，我们需要先计算页面的点击数和展现数，两者相除才能得到该指标。类似的，还有一些转化率、复购率等等。AB测试观测的大部分指标都是比率类指标。
3. **护栏指标**：属于 A/B 测试中基本的合理性检验（Sanity Check），就像飞机起飞前的安全检查一样。它的作用就是作为辅助，来保障 A/B 测试的质量：
   - 衡量 A/B 测试是否符合业务上的长期目标，不会因为优化短期指标而打乱长期目标。
   - 确保从统计上尽量减少出现各种偏差（Bias），得到尽可能值得信任的实验结果。

### 驱动指标一般有什么特点？

1. 易测量的
2. 易归因的
3. 稳定和敏感的

这个地方解释一下稳定和敏感：<font color=lightskyblue>如果实验中的变量变化了，评价指标要能敏感地做出相应的变化；但如果是其他因素变化了，评价指标要能保持相应的稳定性。</font>

以上面的音乐App为例，如果要衡量一次Yoga专辑推送的效果，使用会员续订率这个指标就是不敏感的，他是个长期类指标，因为一次推送只是短期的，所以使用短期的指标比如：Yoga专辑的点击次数，收听率会更加灵敏；

怎么去评判指标的稳定性和敏感性：

1. AA Testing

   如果做AA Testing，两组所选的评价指标差异很大，那可能造成的原因：两组的样本不随机；所选指标不稳定

2. 回溯性分析

   这个分析其实就是AB test知识的沉淀，如果公司之前有做过相关的试验，并且选用了相同的评价指标，就可以看出这个指标是不是敏感

### 如何选取评价指标

1. 结合产品目前所在的阶段

   以 马卡龙App 为例，该产品是个新app，所以会focus在拉新，所以一般选取 拉新过程中的各种点击率、转化率。但是一些成熟的产品，会focus在留存，比如某个功能的使用率、产品平均使用时长、频率等

2. 定性和定量的方法

   一些比较抽象的目标，我们需要使用 **问卷/ 用户调研** + **相关数据分析** 来确定评价指标

   > 曾经看到过一个例子，想找到衡量一款音乐App满意度的指标，首先，通过定性的用户调研，来确定哪些用户满意、哪些用户不满意，完成分组。接着，我们对每组用户（满意的用户和不满意的用户）分别做定量的用户使用习惯的数据分析，发现把音乐收藏到自己曲库的用户有较高的满意度，说明收藏音乐这个行为和用户满意度有强烈的正相关性。这时候，我们就可以把收藏音乐作为评价指标（比如收藏音乐的数量）。更进一步，我们还可以通过数据分析确定“收藏 X 首以上音乐的用户非常满意”中 X 的最优值是多少。

3. 公开和非公开的渠道

### 综合多个指标，建立总体评价标准

当业务有多个目标或者一个目标有多个指标是，需要将他们进行合并观察，构建一个总体评价标准OEC（Overall Evaluation Criteria）。这里面需要注意的一点是，不同指标的单位、大小可能不在一个尺度上，需要先要对各个指标进行归一化（Normalization）处理，使它们的取值都在一定的范围内，比如[0,1]， 之后再进行结合，从而剔除指标单位 / 大小的影响

### 护栏指标

一个AB Test往往容易只关注短期指标，而忽略掉了长期目标，而护栏指标能够保证AB Test的质量

从两个方面来考虑：

1. 用户品质方面
   - 网络延迟：**网页加载时间、App 响应时间等，都是表征网络延迟的护栏指标**。增加产品功能可能会增加网页或 App 的响应时间，而且用户可以敏感地察觉出来。这个时候，就需要在 A/B 测试中加入表征网络延迟的护栏指标，确保在增加产品功能的同时，尽可能减少对用户体验的影响 （一般是通过优化底层代码）。
   - 闪退率
   - 人均指标：收入角度-人均收入  用户参与度-人均使用时长
2. 统计品质方面
   - 样本大小
   - 样本特征：性别、年龄、地理位置

## 实验单位

目前对于随机化单元的选取，通常都是用户粒度：即以一个用户的唯一标识来作为实验样本。好处是符合AB测试的分桶单位唯一性，不会造成一个实验单位处于两个分桶，造成的数据不置信。

